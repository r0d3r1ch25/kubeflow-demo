# Kubeflow Pipelines on k3d - Lightweight Local Setup with Monitoring

A minimal, production-ready Kubeflow Pipelines deployment running on k3d for local development and experimentation. Perfect for ML engineers who want to explore Kubeflow without the complexity of a full cluster setup.

## ğŸ¯ What You Get

- **Kubeflow Pipelines** - Complete ML workflow orchestration
- **Jupyter Notebooks** - Interactive development environment  
- **MinIO** - S3-compatible artifact storage
- **MySQL** - Pipeline metadata storage
- **Argo Workflows** - Robust pipeline execution engine
- **Monitoring Stack** - Promtail + Loki + Grafana for log aggregation and visualization

All running locally with minimal resource usage, tested on Apple Silicon (M-series) Macs.

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install required tools (macOS)
brew install make kubectl k3d kustomize
```

### One-Command Deploy

```bash
make cluster-up
```

That's it! The command will:
1. Create a k3d cluster with proper port forwarding
2. Deploy all Kubeflow components
3. Deploy monitoring stack (Loki, Promtail, Grafana)
4. Wait for everything to be ready
5. Show you the access URLs

### Access Your Services

Once deployed, access these URLs in your browser (using NodePort services):

- **ğŸ”¬ Kubeflow Pipelines UI**: http://localhost:31380
- **ğŸ“Š MinIO Console**: http://localhost:31390 (user: `minio`, pass: `minio123`)
- **ğŸ““ Jupyter Notebook**: http://localhost:31400 (no password required)
- **ğŸ“ˆ Grafana Dashboard**: http://localhost:31410 (user: `admin`, pass: `admin`)

> **Note**: Services are exposed via NodePort on your local machine. The k3d cluster automatically forwards these ports to localhost.

## ğŸ—ï¸ Architecture

This setup provides a complete ML pipeline platform with integrated monitoring:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jupyter       â”‚    â”‚  Kubeflow       â”‚    â”‚     MinIO       â”‚
â”‚   Notebooks     â”‚â—„â”€â”€â–ºâ”‚   Pipelines     â”‚â—„â”€â”€â–ºâ”‚   Storage       â”‚
â”‚   :31400        â”‚    â”‚    :31380       â”‚    â”‚   :31390        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     MySQL       â”‚    â”‚    Grafana      â”‚
                    â”‚   + ML Metadata â”‚    â”‚   + Loki Logs   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    :31410       â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **MySQL 8.0** - Pipeline metadata and experiment tracking
- **MinIO ** - Artifact storage with secret-based authentication
- **Argo Workflows** - Pipeline execution engine
- **ML Pipeline API 2.0** - REST API with v2beta1 support
- **ML Pipeline UI 2.0** - Web interface for pipeline visualization
- **Visualization Server** - Pipeline step visualizations
- **ML Metadata (MLMD)** - Execution tracking and lineage
- **Metadata Envoy** - Proxy for metadata services
- **Persistence Agent** - Workflow completion handling
- **Scheduled Workflow** - Recurring pipeline execution
- **Jupyter Notebook** - Development and experimentation

### Monitoring Components

- **Loki** - Log aggregation system for centralized logging
- **Promtail** - Log collection agent (DaemonSet) that scrapes Kubeflow pod logs
- **Grafana** - Dashboard and visualization with Loki datasource pre-configured

## ğŸ“‹ Available Commands

```bash
# Deploy everything (Kubeflow + Monitoring)
make cluster-up

# Check component status (includes monitoring)
make status

# Delete entire cluster
make cluster-down

# Get Jupyter token (if needed)
make token
```

## ğŸ”§ Development Workflow

1. **Develop** in Jupyter Notebooks at http://localhost:31400
2. **Create** ML pipelines using the Kubeflow Pipelines SDK
3. **Upload** and run pipelines via the UI at http://localhost:31380
4. **Monitor** execution and view artifacts
5. **View logs** in Grafana at http://localhost:31410
6. **Iterate** and improve your ML workflows

## ğŸ“ Project Structure

**Perfect Kubernetes Modular Architecture - One Kind Per File**

```
k8s/
â”œâ”€â”€ base/                            # Core Kubeflow foundation
â”‚   â”œâ”€â”€ rbac/                        # RBAC resources (one per file)
â”‚   â”‚   â”œâ”€â”€ pipeline-runner-serviceaccount.yaml
â”‚   â”‚   â”œâ”€â”€ pipeline-runner-clusterrole.yaml
â”‚   â”‚   â””â”€â”€ pipeline-runner-clusterrolebinding.yaml
â”‚   â”œâ”€â”€ kustomization.yaml           # Base module configuration
â”‚   â””â”€â”€ namespace.yaml               # Kubeflow namespace
â”œâ”€â”€ storage/                         # Data persistence layer
â”‚   â”œâ”€â”€ deployments/                 # All deployment resources
â”‚   â”‚   â”œâ”€â”€ minio-deploy.yaml        # MinIO object storage
â”‚   â”‚   â””â”€â”€ mysql-deploy.yaml        # MySQL metadata database
â”‚   â”œâ”€â”€ services/                    # All service resources
â”‚   â”‚   â”œâ”€â”€ minio-service.yaml       # MinIO API service
â”‚   â”‚   â”œâ”€â”€ minio-console-service.yaml # MinIO console (NodePort 31390)
â”‚   â”‚   â””â”€â”€ mysql-service.yaml       # MySQL service
â”‚   â”œâ”€â”€ secrets/                     # All secret resources
â”‚   â”‚   â””â”€â”€ minio-secret.yaml        # Centralized MinIO credentials
â”‚   â””â”€â”€ kustomization.yaml           # Storage module configuration
â”œâ”€â”€ pipelines/                       # ML Pipeline orchestration
â”‚   â”œâ”€â”€ crds/                        # Custom Resource Definitions
â”‚   â”‚   â”œâ”€â”€ workflows-crd.yaml       # Argo Workflows CRD
â”‚   â”‚   â””â”€â”€ scheduledworkflow-crd.yaml # Scheduled Workflow CRD
â”‚   â”œâ”€â”€ configmaps/                  # All ConfigMap resources
â”‚   â”‚   â””â”€â”€ workflow-controller-configmap.yaml
â”‚   â”œâ”€â”€ deployments/                 # All deployment resources
â”‚   â”‚   â”œâ”€â”€ ml-pipeline-deploy.yaml  # Pipeline API server
â”‚   â”‚   â”œâ”€â”€ ml-pipeline-ui-deployment.yaml # Web interface
â”‚   â”‚   â”œâ”€â”€ ml-pipeline-visualizationserver-deployment.yaml
â”‚   â”‚   â”œâ”€â”€ ml-pipeline-persistenceagent-deployment.yaml
â”‚   â”‚   â”œâ”€â”€ ml-pipeline-scheduledworkflow-deployment.yaml
â”‚   â”‚   â”œâ”€â”€ metadata-grpc-deployment.yaml # ML Metadata GRPC
â”‚   â”‚   â”œâ”€â”€ metadata-envoy-deployment.yaml # ML Metadata proxy
â”‚   â”‚   â””â”€â”€ workflow-controller-deployment.yaml # Argo controller
â”‚   â”œâ”€â”€ services/                    # All service resources
â”‚   â”‚   â”œâ”€â”€ ml-pipeline-service.yaml # Pipeline API service
â”‚   â”‚   â”œâ”€â”€ ml-pipeline-ui-service.yaml # UI service (NodePort 31380)
â”‚   â”‚   â”œâ”€â”€ ml-pipeline-visualizationserver-service.yaml
â”‚   â”‚   â”œâ”€â”€ metadata-grpc-service.yaml
â”‚   â”‚   â””â”€â”€ metadata-envoy-service.yaml
â”‚   â””â”€â”€ kustomization.yaml           # Pipelines module configuration
â”œâ”€â”€ notebooks/                       # Interactive development environment
â”‚   â”œâ”€â”€ deployments/                 # All deployment resources
â”‚   â”‚   â””â”€â”€ jupyter-deployment.yaml  # Jupyter notebook server
â”‚   â”œâ”€â”€ services/                    # All service resources
â”‚   â”‚   â””â”€â”€ jupyter-service.yaml     # Jupyter service (NodePort 31400)
â”‚   â””â”€â”€ kustomization.yaml           # Notebooks module configuration
â”œâ”€â”€ monitoring/                      # Observability and logging stack
â”‚   â”œâ”€â”€ deployments/                 # All deployment resources
â”‚   â”‚   â”œâ”€â”€ loki-deployment.yaml     # Log aggregation system
â”‚   â”‚   â””â”€â”€ grafana-deployment.yaml  # Dashboard and visualization
â”‚   â”œâ”€â”€ services/                    # All service resources
â”‚   â”‚   â”œâ”€â”€ loki-service.yaml        # Loki service
â”‚   â”‚   â””â”€â”€ grafana-service.yaml     # Grafana service (NodePort 31410)
â”‚   â”œâ”€â”€ configmaps/                  # All ConfigMap resources
â”‚   â”‚   â”œâ”€â”€ grafana-configmap.yaml   # Grafana datasource (Loki)
â”‚   â”‚   â””â”€â”€ promtail-configmap.yaml  # Promtail config for Kubeflow logs
â”‚   â”œâ”€â”€ daemonsets/                  # All DaemonSet resources
â”‚   â”‚   â””â”€â”€ promtail-daemonset.yaml  # Log collection agent
â”‚   â”œâ”€â”€ rbac/                        # RBAC resources (one per file)
â”‚   â”‚   â”œâ”€â”€ promtail-serviceaccount.yaml
â”‚   â”‚   â”œâ”€â”€ promtail-clusterrole.yaml
â”‚   â”‚   â””â”€â”€ promtail-clusterrolebinding.yaml
â”‚   â”œâ”€â”€ namespace.yaml               # Monitoring namespace
â”‚   â””â”€â”€ kustomization.yaml           # Monitoring module configuration
â””â”€â”€ kustomization.yaml               # Main modular configuration
```

## ğŸ” Security Features

- **Secret-based Authentication** - All MinIO credentials reference centralized secrets
- **RBAC** - Proper service accounts and role-based access control
- **Namespace Isolation** - Separate namespaces for Kubeflow and monitoring
- **No Hardcoded Passwords** - Credentials managed through Kubernetes secrets


## ğŸ’¡ Use Cases

- **ML Experimentation** - Rapid prototyping of ML workflows
- **Pipeline Development** - Build and test Kubeflow pipelines locally
- **Log Analysis** - Monitor pipeline execution through centralized logging
- **Education** - Learn Kubeflow concepts without cloud complexity
- **CI/CD Testing** - Validate pipelines before production deployment
- **Resource Optimization** - Test pipeline resource requirements

## ğŸ–¥ï¸ System Requirements

**Tested Environment:**
- macOS (Apple Silicon M-series recommended)
- 8GB+ RAM available for Docker
- 12GB+ free disk space

**Resource Usage:**
- ~3GB RAM for all components (including monitoring)
- ~8GB disk space for images and data
- Minimal CPU usage when idle

## ğŸ” Troubleshooting

### Check Component Status
```bash
# All components
make status

# Kubeflow only
kubectl get pods -n kubeflow

# Monitoring only
kubectl get pods -n monitoring
```

### View Logs
```bash
# Pipeline API logs
kubectl logs -n kubeflow -l app=ml-pipeline

# UI logs  
kubectl logs -n kubeflow -l app=ml-pipeline-ui

# Grafana logs
kubectl logs -n monitoring -l app=grafana

# Loki logs
kubectl logs -n monitoring -l app=loki
```

### Reset Everything
```bash
make cluster-down
make cluster-up
```

## ğŸš¦ What's Different

Unlike heavy Kubeflow distributions, this setup:

- âœ… **Lightweight** - Only essential components
- âœ… **Fast startup** - Ready in minutes, not hours
- âœ… **Local-first** - No cloud dependencies
- âœ… **Apple Silicon** - Optimized for M-series Macs
- âœ… **Modular** - Professional Kubernetes structure with proper separation of concerns
- âœ… **Production patterns** - Real MySQL, proper RBAC, secret management
- âœ… **No Istio** - Simplified networking with NodePort access
- âœ… **Integrated Monitoring** - Built-in log aggregation and visualization
- âœ… **Security-focused** - Centralized credential management
- âœ… **Latest MinIO** - Recent 2024 release for better performance
- âœ… **No Duplicates** - Single deployment per service, optimized resource usage
- âœ… **Perfect Kubernetes Structure** - One Kind per YAML file, properly organized by resource type
- âœ… **Zero Mixed Files** - Each YAML contains exactly one Kubernetes resource
- âœ… **Enterprise-Ready** - Follows GitOps and Kubernetes best practices for production

## ğŸ¤ Usage

Feel free to fork this repository or clone it for your own ML experiments.
This setup is designed to be easily customizable for different use cases.

## ğŸ“„ License

MIT License - feel free to use this for your projects.

---

*This setup gets you from zero to running ML pipelines with monitoring in under 10 minutes. Perfect for exploring Kubeflow capabilities without the operational overhead.*